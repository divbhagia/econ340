[
  {
    "objectID": "Problem-Sets/PS2/PS2_Raw/ps2.html",
    "href": "Problem-Sets/PS2/PS2_Raw/ps2.html",
    "title": "Problem Set 2",
    "section": "",
    "text": "################################################################################\n# Problem Set 2\n################################################################################\n\n# For this problem set, you will analyze and combine the following two \n# state-level datasets:\n# - econ_data.xlsx (Economic data)\n# - nbd_data.csv (Neighborhood data)\n# Variables are described in the problem set description on Canvas. \n\n################################################################################\n\n# Uncomment the commands below to load needed packages\n#library(tidyverse) \n#library(readxl) # Install if not already installed \n\n################################################################################\n# 1. Import both Economic and Neighborhood data into R\n################################################################################\n\n# To import .xlsx files:  mydata &lt;- read_excel(\"filename.xlsx\")\n# To import .csv files:   mydata &lt;- read.csv(\"filename.csv\")\n\n# You need to install and load readxl to use read_excel()\n# Look at the names of the files in the folder and replace filename with those names\n# Replace mydata with more intutive names (eg. econ_data, nbd_data)\n# If you are successful, both datasets will appear under data on the top-right\n\n################################################################################\n# 2. Use either the mean() or summarize() function to find the average median \n#    household income (medHHinc) and the average percentage of the population  \n#    living within half a mile of a park (parks).\n################################################################################\n\n\n\n################################################################################\n# 3. Merge both the datasets based on state names.\n################################################################################\n\n# To merge data: merged_data &lt;- merge(data_name1, data_name2, by=\"unit\")\n\n# Replace unit with the name of the variable used for merging. \n# Replace data_name1 and data_name2 with names you assigned to the datasets\n# while importing\n\n################################################################################\n# 4. Find and remove observations with missing values on the variable poverty \n#    from the merged data\n################################################################################\n\n# If a variable var has missing values, the argument is.na(var) returns TRUE. \n\n# Which states had missing values on poverty? Use filter() and is.na() as follows:\n# merged_data %&gt;% filter(is.na(var)==TRUE)\n\n# To remove missing observations, once again use filter() and is.na()\n\n################################################################################\n# 5. Find the state with the highest poverty rate using the functions filter() \n#    and max().\n################################################################################\n\n# Which state has highest poverty rate?\n\n################################################################################\n# 6. Save your resulting dataset in R format. \n#    (R's native format has extension .rda or .Rdata)\n################################################################################\n\n# save(merged_data, file=\"merged_data.rda\") \n# Check if you did indeed save this data in your folder. \n\n################################################################################\n# 7. Use the mutate() and log() functions to create a new variable called   \n#    log_hhinc that is equal to log of medHHinc. Save this new variable to your  \n#    existing merged data. \n################################################################################\n\n\n################################################################################\n# 8. Create a scatter plot with log_hhinc on the x-axis and fine_partc_mttr on  \n#    the y-axis. Use your favorite color for the points on the plot.\n################################################################################\n\n\n################################################################################\n# 9. Use the mutate() function to create a new variable called high_income   \n#    that takes value 1 whenever medHHinc is above 45,000 and 0 otherwise. \n#    Save this new variable to your existing merged data. \n# Note: Income is reported in 1000s of dollars.\n################################################################################\n\n# How many states are categorized as high income? You can use table() or count().\n\n################################################################################\n# 10. Use the functions group_by() and summarize() to find the average fine \n#     particulate matter for states with high_income=1 and high_income=0.\n################################################################################\n\n\n################################################################################\n\n# Make sure to remove all unnecessary comments, such as hints or notes I've provided, \n# so that your final script for submission only contains the essential commands \n# required for executing tasks and any comments you've added."
  },
  {
    "objectID": "Lectures/Lecture 6-8/Slides8.html",
    "href": "Lectures/Lecture 6-8/Slides8.html",
    "title": "ECON 340\nEconomics Research Methods",
    "section": "",
    "text": "Let’s get started\n\n# Load Packages\nlibrary(tidyverse)\n\n# Import data\ndata &lt;- read.csv(\"caschool.csv\")\n\n\n\nFrom Last Class\n\ndata &lt;- data %&gt;% \n  mutate(hcomp = ifelse(comp_stu&gt;=median(comp_stu),1,0))\n\n\nSyntax: ifelse(test_expression, x, y)\nThe returned vector has element from x if the corresponding value of test_expression is TRUE and y if it is FALSE\nSo here hcomp takes value 1 whenever computers per student are above the median, and 0 otherwise. What should be the output from mean(data$hcomp)?\n\n\n\nFactor Variables\n\nVariables can be continous (like testscr) or discrete (like hcomp and gr_span)\nWhen the categorical variable is numeric (like hcomp) sometimes it is useful to store it as a factor variable\nThis helps prevent R from treating it as a continous variable\n\n\n\nFactor Variables\nTo factorize hcomp\n\ndata$hcompf &lt;- factor(data$hcomp, \n                      levels = c(0,1),\n                      labels = c(\"Low\", \"High\"))\n\nOr simply,\n\ndata$hcompf2 &lt;- factor(data$hcomp)\n\n\n\nGraphs using ggplot\n\nggplot2 is an R package included with TidyVerse for data visualization (alternative to base R’s plot())\nggplot2 is designed to work iteratively\nYou start with a layer and then add layers (using +s) of annotations and statistical summaries\n\n\n\nHistogram: Discrete Variable\n\nggplot(data=data, aes(x=gr_span)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\nHistogram: Continous Variable\n\nggplot(data=data, aes(x=str)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\nMaking things pretty\n\nggplot(data=data, aes(x=str)) +\n  geom_histogram(color=\"black\",fill=\"blue\",alpha=0.25) +\n  labs(x=\"Student-Teacher Ratio\", y=\"Count\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nScatterplot\n\nggplot(data, aes(x=str, y=testscr)) +\n  geom_point(shape=1) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nSaving output\n\nTo save graphs produced by ggplot(), use ggsave()\nSaves the last graph that was produced in your working directory unless specified otherwise\nLet’s try it\n::: {.cell layout-align=“center”}\nggsave(\"myplot.png\")\n:::\nCan even specify the height and width of our graphs\n::: {.cell layout-align=“center”}\nggsave(\"myplot.png\", width = 4, height = 3)\n:::\n\n\n\nAnother Useful Package: Stargazer\n\nlibrary(stargazer)\ndata %&gt;%\n  select(testscr, str, comp_stu, meal_pct) %&gt;%\n  stargazer(type = 'text')\n\n\n==============================================\nStatistic  N   Mean   St. Dev.   Min     Max  \n----------------------------------------------\ntestscr   420 654.157  19.053  605.550 706.750\nstr       420 19.640   1.892   14.000  25.800 \ncomp_stu  420  0.136   0.065    0.000   0.421 \nmeal_pct  420 44.705   27.123   0.000  100.000\n----------------------------------------------\n\n\n\n\nReal Data is Messier\n\nMissing values are stored as NA in R\nIf a value is missing, mean() and other functions will give an error, so use option na.rm=TRUE\nlogical is.na() returns TRUE if value is missing and FALSE otherwise\nSo you can use filter() as follows to delete missing values on some variable var\n\ndata  &lt;- data  %&gt;% \n  filter(is.na(var) = FALSE)\n\n\n\n\nReal Data is Messier\n\nOften we need to combine two different datasets\nTo merge two data sets on the basis of a common unit, use merge()\nTo append data sets, say, across years, can use rbind()\nProblem set 2 will have you deal with some of this\nNext week: Back to theory"
  },
  {
    "objectID": "Lectures/Lecture 6-8/Slides7.html",
    "href": "Lectures/Lecture 6-8/Slides7.html",
    "title": "ECON 340\nEconomics Research Methods",
    "section": "",
    "text": "So far\n\n# Load Packages\nlibrary(tidyverse)\n\n# Set directory (optional)\n#setwd(\"~/Dropbox (CSU Fullerton)/Econ340_R\")\n\n# Import data\ndata &lt;- read.csv(\"caschool.csv\")\n\nYou can clear your environment before starting by using the broom on the top-right. Or add rm(list=ls()) command on top of your R-script.\n\n\nTabulating Variables\n\nVariable gr_span reports the grade span of a school district (K-6 or K-8)\n\ntable(data$gr_span)\n\n\nKK-06 KK-08 \n   61   359 \n\n\nSo 61 school districts go up to grade 6 while 359 go up to grade 8\n\n\n\nDplyr Syntax\n\ndplyr is a TidyVerse package that provides several useful functions for data manipulation\nHowever, dplyr uses slightly different syntax from base R.\nOne key operator utilized by this package is the pipe operator %&gt;%\nYou can use shortcut Cmd + Shift + M (Mac) and Ctrl + Shift + M (Windows) for %&gt;%\nYou can think of this operator as standing for “then” in the code\n\n\n\nDplyr Syntax\nFor example, to tabulate data:\n\ndata %&gt;% count(gr_span)\n\n  gr_span   n\n1   KK-06  61\n2   KK-08 359\n\n\n\n\nSome Useful dplyr Functions\n\nmutate() adds new variables that are functions of existing variables\nselect() picks variables based on their names\nfilter() picks cases based on their values\nsummarise() reduces multiple values down to a single summary\narrange() changes the ordering of rows\ngroup_by() performs subsequent calculations within-group (and ungroup() when done)\n\n\n\nSelect Variables\n\ndata %&gt;% select(computer, enrl_tot)\n\n    computer enrl_tot\n1         67      195\n2        101      240\n3        169     1550\n4         85      243\n5        171     1335\n6         25      137\n7         28      195\n8         66      888\n9         35      379\n10         0     2247\n11        86      446\n12        56      987\n13        25      103\n14         0      487\n15        31      649\n16        80      852\n17       100      491\n18        50      421\n19       960     6880\n20       139     2688\n21        69      440\n22        53      475\n23       169     2538\n24         0      476\n25       216     2357\n26       198     1588\n27       742     7306\n28       269     2601\n29        67      847\n30        55      452\n31       569     4142\n32       224     2102\n33       721    10012\n34       202     2488\n35      1713    25151\n36       177     2267\n37       204     1657\n38        18      284\n39       562     5370\n40       275     2471\n41      1762    15386\n42        40      184\n43        78     1217\n44       571     6219\n45       324     4258\n46       175     1235\n47      1423    16244\n48        85      814\n49      3324    27176\n50      1306    10696\n51       786     8935\n52       242     1600\n53       669     9028\n54       896    10625\n55       560     7151\n56       202     2404\n57       480     5804\n58       196     2253\n59       152     2807\n60       249     3074\n61        45      723\n62       560     5138\n63      1048    20927\n64       496     3017\n65       149      957\n66       287     1639\n67       284     4340\n68       585     5079\n69       505     6639\n70       129     1154\n71        13      237\n72       273     2987\n73        94      499\n74      1248    11474\n75       107     1088\n76       259     2660\n77         8      353\n78        37      329\n79        32      252\n80        31      175\n81       897     3835\n82         8      314\n83       445     4458\n84       112     1313\n85        36      474\n86       277     1114\n87       178     1358\n88      1015    11629\n89      1261     6195\n90        35      499\n91        27      417\n92        55      300\n93        50      457\n94        34      146\n95        71      460\n96        28      354\n97       315     1841\n98       717     3760\n99        70      500\n100      383     5112\n101       28      146\n102      134     2141\n103      144      610\n104       26      337\n105      597     4501\n106      313     5718\n107     1182    19402\n108      560     3401\n109      397     2621\n110       87      426\n111       24      205\n112     1321    13668\n113       98      342\n114      499     6518\n115       30      239\n116      281     2911\n117     1338     6272\n118     1435    10218\n119      222     1735\n120       96      474\n121      145      544\n122      283     1987\n123       50      418\n124       40      196\n125       92     2208\n126      165     1255\n127       65     1469\n128      680     7114\n129      170     1962\n130      658     7761\n131       47      216\n132       31      224\n133     1004     7887\n134        0      752\n135     1719     9328\n136       35      548\n137       19      104\n138        4      275\n139       52      443\n140     1268    10337\n141       36      806\n142       25      227\n143     1333     8416\n144       30      149\n145       48      220\n146      601     4612\n147       71      590\n148       25      133\n149      116     2440\n150        0      133\n151       65      519\n152       30      222\n153       36      285\n154      471     3129\n155      167     2019\n156      316     5620\n157     1218     9775\n158       38      246\n159      800     7210\n160     2889    21338\n161       27      477\n162       50      727\n163       46      374\n164     2001    18255\n165     1058     8787\n166       60      797\n167       40      140\n168       51      235\n169     1295     8294\n170      354     2409\n171       16      150\n172      606     3981\n173      345     2326\n174      163      501\n175       48      470\n176       59      575\n177      789     3519\n178       61      474\n179       60      223\n180       22       92\n181      500     4971\n182      226     2617\n183       32      242\n184       75      780\n185       50      324\n186       25      140\n187       26      181\n188       65      516\n189       25      108\n190       58      419\n191     1099    12567\n192       65      287\n193      634     6201\n194      134      577\n195       41      170\n196       36      164\n197       75      382\n198      250     1221\n199      213     2214\n200      409     4523\n201       49      793\n202      254     1678\n203       35      536\n204       36      307\n205       56      347\n206       30      168\n207       81      532\n208      374     3272\n209      268     2045\n210        7      156\n211      164     1129\n212      500     3669\n213       19      157\n214      618     4928\n215       13      103\n216       17      175\n217      241     4153\n218       67      280\n219       96      865\n220     1120     8735\n221       72      412\n222      624     6373\n223       73      332\n224      192     2903\n225       60      565\n226       78      586\n227      626     5068\n228      118      859\n229       20      145\n230      155      649\n231      435     1789\n232       50      775\n233      148      777\n234      419     3518\n235     2401    19294\n236     1158     7661\n237       34      158\n238       42      117\n239       37      160\n240       30      511\n241      296     2770\n242       46      551\n243      474     5205\n244      622     6437\n245      122     1712\n246       56      370\n247      328     3182\n248       32      139\n249     1507    11855\n250      129     1068\n251      224     2295\n252      141     1510\n253       60      579\n254      140     1012\n255      107     1212\n256       23      119\n257       78      590\n258       60      546\n259       51      248\n260      150      461\n261      496     6312\n262       62      285\n263      302     2325\n264     1790    11885\n265       46      564\n266     1671    12380\n267      529     3772\n268       70      895\n269      489     5714\n270       16      105\n271      125     1449\n272       95      510\n273       39      160\n274       50      433\n275      411     3186\n276      576     5010\n277      100      717\n278      335     3548\n279       78      868\n280       60      507\n281      105      822\n282      143     1792\n283      190     1202\n284       58      515\n285      263     1354\n286      113     1252\n287      127      823\n288      258     2231\n289       81      271\n290       85      309\n291      308     3005\n292      114      966\n293      814     7710\n294       54      762\n295      206     1708\n296     1113     9850\n297       10      129\n298     1150    10619\n299      494     4521\n300      114      580\n301      326     2569\n302      756     6022\n303       47      670\n304     1962    14708\n305      706     6601\n306       87      675\n307      193     2458\n308       14      144\n309       48      573\n310       82      721\n311      160      992\n312        0     8432\n313       15      244\n314      785     7116\n315       95      830\n316       38      160\n317      220     1588\n318      307     2272\n319       65     1425\n320       30      245\n321      259     1349\n322       65      400\n323      637     4632\n324       15      224\n325       92      576\n326       75      451\n327      145      900\n328       10      118\n329        0     1457\n330      586     4734\n331      379     3303\n332      540     6055\n333       39      424\n334      615     2801\n335       37      187\n336       17      129\n337       41      188\n338      116     1212\n339      249     2596\n340      753     4925\n341      930     6257\n342      122      868\n343      934     3787\n344      855     6423\n345       47      678\n346       23      162\n347      834     8529\n348      231     1862\n349      183     1452\n350       32      155\n351      343     2536\n352      103      567\n353      135      953\n354       42      296\n355       24      198\n356       49      218\n357      117      734\n358       50      189\n359      482     2528\n360      290     2987\n361       21      208\n362       29      379\n363       27      145\n364      103      706\n365      105      878\n366       75      594\n367       40      139\n368      318     2089\n369       42      326\n370       50      516\n371       50      449\n372       70      297\n373      165     1579\n374       49      383\n375       14       81\n376      701     5259\n377      223     1960\n378       31      151\n379      310      946\n380      300     2707\n381      141      919\n382      216      945\n383      220      738\n384       35      164\n385       29      167\n386       25      125\n387      113     1091\n388       27      134\n389       55      600\n390      283     1803\n391       15      158\n392      439     2392\n393       33      526\n394       44      141\n395       30      235\n396      406     3280\n397        0     1254\n398      155      948\n399     2232    15228\n400       21       81\n401      540     2768\n402       84      535\n403      404     2542\n404      265     1940\n405      272     1059\n406      403     2340\n407      496     3469\n408      300     2106\n409      112      478\n410      241     1885\n411      466     2422\n412      412     1318\n413       22      220\n414      209      687\n415      286     2341\n416      195      984\n417      721     3724\n418       45      441\n419       14      101\n420      313     1778\n\n\n\n\nFinding Correlation\n\n# Base R\ncor(data$computer, data$enrl_tot)\n\n[1] 0.9288821\n\n\n\n# Tidy way\ndata %&gt;% select(computer, enrl_tot) %&gt;% cor() \n\n          computer  enrl_tot\ncomputer 1.0000000 0.9288821\nenrl_tot 0.9288821 1.0000000\n\n\n\n\nFilter Observations\n\ndata %&gt;% select(gr_span, computer) %&gt;% \n  filter(gr_span==\"KK-06\")  \n\n   gr_span computer\n1    KK-06        0\n2    KK-06      742\n3    KK-06      324\n4    KK-06      669\n5    KK-06      196\n6    KK-06      560\n7    KK-06     1048\n8    KK-06      505\n9    KK-06      129\n10   KK-06      178\n11   KK-06      499\n12   KK-06     1435\n13   KK-06       65\n14   KK-06     1268\n15   KK-06     2889\n16   KK-06     1295\n17   KK-06      354\n18   KK-06       16\n19   KK-06       59\n20   KK-06      500\n21   KK-06       65\n22   KK-06       67\n23   KK-06      474\n24   KK-06       32\n25   KK-06       23\n26   KK-06      576\n27   KK-06      143\n28   KK-06       58\n29   KK-06      113\n30   KK-06      258\n31   KK-06       81\n32   KK-06      308\n33   KK-06      494\n34   KK-06      326\n35   KK-06       38\n36   KK-06      220\n37   KK-06       30\n38   KK-06      145\n39   KK-06      586\n40   KK-06      540\n41   KK-06       41\n42   KK-06      834\n43   KK-06       32\n44   KK-06      117\n45   KK-06       50\n46   KK-06      701\n47   KK-06       31\n48   KK-06      300\n49   KK-06      216\n50   KK-06       35\n51   KK-06       29\n52   KK-06       27\n53   KK-06       15\n54   KK-06       44\n55   KK-06       30\n56   KK-06        0\n57   KK-06       21\n58   KK-06       84\n59   KK-06      404\n60   KK-06      300\n61   KK-06       22\n\n\n\n\nAnd and Or in R\nTo select schools in Orange county with enrollment over 5000\n\ndata1 &lt;- data %&gt;% \n  filter(county==\"Orange\" & enrl_tot&gt;=5000)\n\nTo select schools that are either in Orange country or in LA county\n\ndata2 &lt;- data %&gt;% \n  filter(county==\"Orange\" | county==\"Los Angeles\")\n\n\n\nSummarize Variables\n\n# Calculating mean\ndata %&gt;% summarise(mean(computer))\n\n  mean(computer)\n1       303.3833\n\n# Standard deviation and median\ndata %&gt;% summarise(sd = sd(computer), \n                   med = median(comp_stu))\n\n        sd       med\n1 441.3413 0.1254644\n\n\n\n\nCreating New Variables\n\ndata &lt;- data %&gt;% \n  mutate(log_enrl = log(enrl_tot))\n\n\nThe code takes data and adds a new column log_enrl, which is the log of enrl_tot\nIt then updates the original data with this new column.\n\n\n\nCreating New Variables\n\ndata &lt;- data %&gt;% \n  mutate(hcomp = ifelse(comp_stu&gt;=median(comp_stu),1,0))\n\n\nSyntax: ifelse(test_expression, x, y)\nThe returned vector has element from x if the corresponding value of test_expression is TRUE and y if it is FALSE\nSo here hcomp takes value 1 whenever computers per student are above the median, and 0 otherwise. What should be the output from mean(data$hcomp)?\n\n\n\nCombining group_by() and summarise()\n\ndata %&gt;% \n  group_by(hcomp) %&gt;% \n  summarise(mean(comp_stu))\n\n# A tibble: 2 × 2\n  hcomp `mean(comp_stu)`\n  &lt;dbl&gt;            &lt;dbl&gt;\n1     0           0.0881\n2     1           0.184 \n\n\n\n\nExcercise for you\nFind the county with the highest average number of computers per student (comp_stu) (Hint: Use group_by(county) and summarise())\n\n\nWhat’s next?\n\nNext class we will continue with R\nProblem set 2 is available on Canvas and is due next Tuesday (09/19)\nGet in touch with your research group partner(s) and start thinking about your project\nResearch Paper First Submission is due in two weeks (09/26)"
  },
  {
    "objectID": "Lectures/Lecture 21-22/Slides22.html",
    "href": "Lectures/Lecture 21-22/Slides22.html",
    "title": "ECON 340\nEconomics Research Methods",
    "section": "",
    "text": "Housekeeping\n\nrm(list=ls())\nlibrary(tidyverse)\nlibrary(stargazer)\n#setwd(\"~/Dropbox (CSU Fullerton)/Econ340_R\")\ndata &lt;- read.csv(\"acs2019.csv\")\n\n\n\nPreparing the data\n\n# Select sample and variables\ndata &lt;- data %&gt;% \n  filter(empstat==1) %&gt;% \n  select(-fertyr, -rent)\n\n# Remove missing values\ndata &lt;- na.omit(data)\n\n\n\nHourly wage and age\n\nggplot(data, aes(x=age, y=hrly_wage)) +\n  geom_point() + theme_classic()\n\n\n\n\n\n\n\n\n\n\nHourly wage and age\n\nToo much data to make sense\nBetter to plot average hourly wage at each wage\nuse stat_summary() and specify fun as mean\n\n\n\nAverage wages by age\n\nggplot(data, aes(x=age, y=hrly_wage)) +\n      stat_summary(fun = mean, geom = \"point\") + \n      theme_classic()\n\n\n\n\n\n\n\n\n\n\nHourly wage and age\n\nTo fit a quadratic model, generate age-squared term\n::: {.cell layout-align=“center”}\ndata &lt;- data %&gt;% \n  mutate(age.sq = age*age)\n:::\nFit linear and quadratic model\n::: {.cell layout-align=“center”}\nmdl.lnr &lt;- lm(hrly_wage ~ age, data)\nmdl.qdr &lt;- lm(hrly_wage ~ age + age.sq, data)\n:::\nOutput using stargazer()\n::: {.cell layout-align=“center”}\nstargazer(mdl.lnr, mdl.qdr, type=\"text\")\n:::\n\n\n\nHourly wage and age\n\n\n\nPlotting the fitted curve\n\ndata$prd.qdr &lt;- predict(mdl.qdr)\nggplot(data, aes(x=age, y=hrly_wage)) +\n      stat_summary(fun = mean, geom = \"point\") + \n      geom_line(aes(y=prd.qdr)) + theme_classic()\n\n\n\n\n\n\n\n\n\n\nDummy variables\n\ndata %&gt;% group_by(female) %&gt;% \n  summarise(avg_wages=mean(hrly_wage))\n\n# A tibble: 2 × 2\n  female avg_wages\n   &lt;int&gt;     &lt;dbl&gt;\n1      0      31.3\n2      1      25.8\n\n\n\n\nDummy variables\n\nmdl1 &lt;- lm(hrly_wage ~ female, data)\nmdl2 &lt;- lm(hrly_wage ~ female + yrs_educ, data)\nmdl3 &lt;- lm(hrly_wage ~ female*yrs_educ, data)\n\nstargazer(mdl1, mdl2, mdl3, type=\"text\")\n\n\n\nDummy variables\n\n\n\nModel 2\n\ndata$prd2 &lt;- predict(mdl2)\nggplot(data, aes(x=yrs_educ, y=prd2, group=female)) +\n  geom_line(aes(color=female)) + theme_classic()\n\n\n\n\n\n\n\n\n\n\nFactor variables\n\nR thinks of all variables as numeric unless you tell it otherwise\nTo create a factor variable (specifying levels and labels is optional)\n::: {.cell layout-align=“center”}\ndata$fem.fct &lt;- factor(data$female, levels = c(0,1), \n                   labels = c('Female', 'Male'))\ntable(data$fem.fct)\n::: {.cell-output .cell-output-stdout} ```\nFemale Male 8886 8223 ``` ::: :::\n\n\n\nModel 3\n\ndata$prd3 &lt;- predict(mdl3)\nggplot(data, aes(x=yrs_educ, y=prd3, group=fem.fct)) +\n  geom_line(aes(color=fem.fct)) + theme_classic()\n\n\n\n\n\n\n\n\n\n\nMore Interaction Terms\n\nmdl.int1 &lt;- lm(hrly_wage ~ female*married, data)\nstargazer(mdl.int1, type=\"text\")\n\n\n\nMore Interaction Terms\n\nmdl.int2 &lt;- lm(hrly_wage ~ black*female, data)\nstargazer(mdl.int2, type=\"text\")\n\n\n\nVariables with several categories\n\n# Specify levels and labels\nlevs &lt;- c(1, 2, 3, 4, 5)\nlabs &lt;- c(\"Less than HS\", \"High School\", \n          \"Some College\", \"College Degree\", \n          \"More than College\")\n\n# Create factor variable\ndata$educ.fct &lt;- factor(data$educ_cat, \n                        levels=levs, labels=labs)\n\n\n\nVariables with several categories\n\ndata %&gt;% group_by(educ.fct) %&gt;%\n  summarise(m = mean(hrly_wage))\n\n# A tibble: 5 × 2\n  educ.fct              m\n  &lt;fct&gt;             &lt;dbl&gt;\n1 Less than HS       17.5\n2 High School        20.6\n3 Some College       23.7\n4 College Degree     34.2\n5 More than College  43.5\n\n\n\n\nVariables with several categories\n\nWant to specify to R to treat education as a categorical variable\nWhich of the following models is correct?\n::: {.cell layout-align=“center”}\nsummary(lm(hrly_wage ~ educ_cat, data))\nsummary(lm(hrly_wage ~ as.factor(educ_cat), data))\nsummary(lm(hrly_wage ~ educ.fct, data))\n:::\nCoefficients capture mean differences from the baseline\n\n\n\nVariables with several categories\n\n\n\nLog Transformations\n\nCreate transformed variable\n::: {.cell layout-align=“center”}\ndata$lwage &lt;- log(data$hrly_wage)\n:::\nFit the model and output results\n::: {.cell layout-align=“center”}\nmdl.lnr &lt;- lm(hrly_wage ~ yrs_educ, data)\nmdl.log &lt;- lm(lwage ~ yrs_educ, data)\nstargazer(mdl.lnr, mdl.log, type=\"text\")\n:::\n\n\n\nLog Transformations\n\n\n\nLog Transformations\nHow much does hourly wage change going from 10 to 11 years of education?\n\nLinear model: $2.52\nLog-level model: 100 \\(\\times\\) 0.086 = 8.6% of $18.87 = $1.68\n::: {.cell layout-align=“center”}\ndata %&gt;% filter(yrs_educ==10) %&gt;% \n  summarise(m = mean(hrly_wage))\n::: {.cell-output .cell-output-stdout} m   1 18.87474 ::: :::\n\n\n\nLog Transformations\n\nWhat about going from 13 to 14 years of education?\nFitting a linear model between log wages and years of education \\(\\rightarrow\\) non-linear model between wages and years of education\n::: {.cell layout-align=“center”}\n# Predictions from the log-level model\ndata$lw.hat &lt;- predict(mdl.log) \n\n# Convert predictions back to levels\ndata$w.hat &lt;- exp(data$lw.hat)\n:::\n\n\n\nLog Transformations\n\nggplot(data, aes(x=yrs_educ, y=w.hat)) +\n  geom_line() + theme_classic()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Economic Research Methods",
    "section": "",
    "text": "Master statistical and econometric tools along with coding in R, to analyze data and answer economic questions.\n\nECON 340 | Fall 2023 | College of Business and Economics | California State University\n\n\nInstructor\n   Dr. Div Bhagia     Office: SGMH 3361     dbhagia@fullerton.edu \n\n\nCourse Details\n   SGMH 2501     Tuesdays and Thursdays, 2:30-3:45 PM     Office hours:\n\n\nThursdays, 4-6.45 PM, or\nBy appointment (in-person or on Zoom)"
  },
  {
    "objectID": "content.html",
    "href": "content.html",
    "title": "Lecture Materials",
    "section": "",
    "text": "Module I: Describing Data\nLecture Notes\n\nLecture 1: Introduction; Summation Notation (slides, class handout)\nLecture 2: Distribution, Mean, Median, Percentiles (slides, class handout)\nLecture 3: Variance, Standard Deviation, Z-Score (slides, class handout)\nLecture 4: Covariance and Correlation (slides, class handout)\nLecture 5: Research Questions and Data (slides)\n\n\n\nModule II: Coding in R\n\nLecture 6: Getting Started with R (slides)\nLecture 7: Data Analysis in R (slides)\nLecture 8: Data Analysis in R (slides)\n\n\n\nModule III: Random Variables\nLecture Notes\n\nLecture 9: Distribution, Expectation, Variance (slides, class handout)\nLecture 10: Normal Distribution, Z-Score (slides, class handout)\nLecture 11: Independence and Correlation (slides, class handout)\n\n\n\nModule IV: Sampling and Estimation\nLecture Notes\n\nLecture 12: Good Estimators, Sample Mean Distribution (slides, class handout)\nLecture 13: Confidence Intervals (slides, class handout)\nLecture 14: Hypothesis Testing and P-Values (slides, class handout)\n\n\n\nModule V: Linear Regression\nNotes for Lectures 15-17\n\nLecture 15: Ordinary Least Squares (OLS); R-Squared (slides, class handout)\nLecture 16: Prediction vs. Causal Inference (slides)\nLecture 17: Inference (p-values, t-stats, confidence intervals) (slides, class handout)\nLecture 18: Omitted Variable Bias; Multiple Regression (slides, class handout, lecture notes I & II)\nLecture 19: Categorical Variables; Interaction Terms (slides, class handout, lecture notes)\nLecture 20: Quadratic and Log Functional Forms (slides, class handout, lecture notes)\nLecture 21: Regression Analysis in R (slides)\nLecture 22: Regression Analysis in R (slides)\n\n\n\nModule VI: Advanced Topics\n\nLecture 23: Experiments and Quasi-Experimental Methods (slides)\nLecture 24: Differences-in-Differences (slides)\nLecture 25: Big Data and Machine Learning (slides)"
  },
  {
    "objectID": "research-project.html",
    "href": "research-project.html",
    "title": "Research Project",
    "section": "",
    "text": "Overview\nAs a part of this class, you will write an empirical research paper using R in groups of 2-3 students. You will pick a question and a dataset and use the tools from this class to answer your question. Over the semester, you will need to meet the following milestones:\n\nSep 26: First submission (5%)\nOct 31: Second submission (10%)\nDec 05: Final submission (15%)\n\nThe goal of the first submission is to make you pick your data and question. For the second submission, you will be performing preliminary data analysis. The final paper will be 7 to 10 pages and will thoroughly discuss all the analyses you performed. The paper should demonstrate your writing ability and ability to perform and interpret statistical analyses correctly.\nThe following links provide detailed instructions for each submission:\n\nFirst submission (instructions, template, example)\nSecond submission (instructions, example)\nFinal submission (instructions)\n\n\n\nQuestions and Data\nYou can pick any question and data as long as it meets the requirements specified in Submission 1. I have also compiled a set of datasets for you to use for your research projects. You can find these in the Dropbox folder: Econ340 Datasets. Let me know if you have any issues accessing the folder. Here is a list of the datasets in the Dropbox folder. You can pick a dataset from the Dropbox folder or use an external one. Below are some external sources of data. \n\n\nExternal Data Sources\n\nThe US Bureau of Labor Statistics (BLS) maintains lots of series.  A notable one is the Local Unemployment Area Statistics (LUAS) which provides the unemployment rate for US metropolitan areas.\nThe US Bureau of Economic Analysis also maintains lots of series. (You can look for data by Topic or by Place)\nWorld Bank provides large datasets across countries, including the World Development Indicators (WDI) database and the Global Financial Development Database (GFDD). \nIPUMS is your one-stop shop for downloading US (and international) microdata, such as from the Current Population Survey (CPS), American Community Survey (ACS), American Time-Use Survey (ATUS), etc.\nTwo leading US longitudinal datasets are the Panel Study of Income Dynamics (PSID) and National Longitudinal Surveys (NLSY).\nCounty Business Patterns (CBP) data contains the number of establishments and employment for different industries in each US county.\nFRED at the Federal Reserve Bank of St. Louis maintains numerous data series on macroeconomic variables (mostly time series).\nOpportunity Insights Data Library has some datasets at the county or commuting zone level with variables related to economic mobility. \nMore county-level data is available from USDA ERS. \nCheck out FiveThirtyEight for sports and election data.\nIf you are looking for more data, check out AEA’s Compiled Data Sources (Check this link too).\n\nFinally, if you need some inspiration to come up with a research question, here are some links to get you thinking:\n\nFreakonomics Radio\nAEA Research Digest\nNBER Research News\nVoxEU\nIMF Blog\n\nNewspapers and other news outlets are also good sources to learn about current issues."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Economic Research Methods",
    "section": "",
    "text": "Problem Sets\n\nProblem Set 1\nProblem Set 2 (Download zippped folder with datasets and R Code to complete.)\nProblem Set 3\nProblem Set 4\n\n\n\nExams Related Material\n\nMidterm\n\nSample Exam\nSample Exam Solutions\nStudy Guide\nFormula Sheet \n\nFinal\n\nSample Exam\nSample Exam Solutions\nStudy Guide\nSample Questions for Module VI"
  },
  {
    "objectID": "Compiled/Research Paper/example_code.html",
    "href": "Compiled/Research Paper/example_code.html",
    "title": "Submission 2 Example Code",
    "section": "",
    "text": "### Housekeeping\nrm(list = ls()) # Clear workspace (optional)\nlibrary(tidyverse)\nlibrary(stargazer) \n\n################################################################################\n# Load and prepare data\n################################################################################\n\n# Load data\ndata &lt;- read.csv(\"caschool.csv\")\n\n# Create high_comp_stu\ndata &lt;- data %&gt;% \n  mutate(high_comp_stu = ifelse(comp_stu&gt;=median(comp_stu),1,0))\n\n# Select variables and sample\nfinaldata &lt;- data %&gt;% \n  filter(gr_span==\"KK-08\") %&gt;% \n  select(testscr, str, high_comp_stu, meal_pct)\n\n################################################################################\n# Create Summary Statistics Table\n################################################################################\n\nfinaldata %&gt;%\n  as.data.frame() %&gt;% \n  stargazer(type = 'text', digits = 2, median=TRUE)\n\n# If you copy and paste the table into Word, make sure to select a fixed width \n# font like Courier New. You can also take a screenshot and insert that into \n# your document. \n\n################################################################################\n# Relationship between test score and student-teacher ratio\n################################################################################\n\n# Scatter plot (optional: geom_smooth to add linear fit)\nggplot(finaldata, aes(y=testscr, x=str))+\n  geom_point(shape=1) +\n  #geom_smooth(method=lm, se=FALSE, size=0.5, color=\"black\") +\n  labs(y=\"Test Score\", x=\"Student-Teacher Ratio\")+\n  theme_classic()\nggsave(\"scatter.png\", width = 5, height=3.5)\n\n# Correlation\ncorr_tst_str &lt;- cor(finaldata$testscr, finaldata$str)\n\n\n######## If my primary independent variable was categorical, I would need a bar plot \n\ntab &lt;- finaldata %&gt;% \n  group_by(high_comp_stu) %&gt;% \n  summarise(testscr = mean(testscr))\n\nggplot(tab, aes(y=testscr, x=as.factor(high_comp_stu)))+\n  geom_bar(stat=\"identity\") +\n  theme_classic()\n\n################################################################################\n# Correlation Table\n################################################################################\n\n# If any of your variables are categorical, convert them to numerical variables\n# before finding the correlations\n\nfinaldata %&gt;% \n  select(testscr, str, meal_pct, high_comp_stu) %&gt;% \n  cor() %&gt;% \n  stargazer(type = 'text', digits = 2)\n\n################################################################################"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "The key components of the syllabus are outlined below. You can access the complete syllabus in PDF format by clicking here.\n\nOverviewLearning GoalsCourse StructureMaterialGrading\n\n\n\nOverview\nThis course covers the basics of conducting quantitative economic research. The course aims to take you through the steps involved in answering a research question using observational data. You will learn and implement statistical and econometric concepts vital to empirical research. You will select a question, locate data to answer it and use the tools we learn in this class to answer this question.\nThis course will involve hands-on work with data using R, a statistical software, both inside and outside the classroom. The tools learned in this class will be helpful regardless of whether your goal is to be a researcher, a consultant, run your own business, or work for a non-profit.\n\n\n\n\nLearning Goals\nUpon successful completion of this course, students will be able to discern valuable insights from datasets and communicate empirical findings effectively. In particular, you will\n\nDevelop a strong grasp of both the conceptual and practical aspects of various statistical and econometric tools.\nLearn to tidy, wrangle, manipulate, and visualize data using TidyVerse in R.\nBe able to compute descriptive statistics, perform regression analysis in R, and present results in a clear, elegant manner.\nGain the skill to effectively communicate empirical findings.\nDevelop an understanding of causality, including the ability to identify and articulate potential threats to causal inference\nGet an introduction to advanced topics at the forefront of economic research, such as quasi-experimental methods and machine learning.\n\n\n\n\n\nCourse Structure\nAll meetings for this are anticipated to take place in a physical classroom setting. During our class sessions, I use lecture slides to deliver the day’s topics, and we frequently engage in collaborative problem-solving using handouts, practical work in R, or participate in class discussions. The course is designed to be interactive, and requires you to actively participate. Therefore, regular class attendance is especially important for you to suceed in this class.\nA substantial portion of this class centers around the development of a research paper. You have the freedom to choose your dataset or you can utilize one of the datasets that I have compiled for this class in the Econ340 Datasets Dropbox folder. You will start working on the research project early in the semester with an assignment focused on crafting the paper’s introduction and conducting descriptive analysis. I will provide guidance and support as you progress through the semester, and also schedule a mid-semester meeting to evaluate your progress\n\n\n\n\nCourse Material\nAll course materials—including lecture slides, handouts, notes for each topic—are available on the course website. These materials should generally be sufficient, and there is no mandatory textbook for this class. However, if you have a keen interest in the subject and seek additional references, the following options are excellent choices:\n\nStock J, Watson M. Introduction to Econometrics (3rd edition). Addison Wesley Longman; 2011.\nHuntington-Klein, Nick. 2021. The Effect: An Introduction to Research Design and Causality. Boca Raton, Florida: Chapman and Hall / CRC. https://theeffectbook.net/\n\n\n\n\n\nGrading Criteria\nYour course grade will be determined according to the following breakdown:\n\n\n\n\nComponent\n\n\nPoints\n\n\n\n\nActive Engagement\n\n\n10\n\n\n\n\nProblem Sets\n\n\n20\n\n\n\n\nResearch Paper: First Submission\n\n\n10\n\n\n\n\nResearch Paper: Final Submission\n\n\n20\n\n\n\n\nMidterm Exam\n\n\n20\n\n\n\n\nFinal Exam\n\n\n20"
  },
  {
    "objectID": "Lectures/Lecture 21-22/Slides21.html",
    "href": "Lectures/Lecture 21-22/Slides21.html",
    "title": "ECON 340\nEconomics Research Methods",
    "section": "",
    "text": "Housekeeping\n\nrm(list=ls())\nlibrary(tidyverse)\nlibrary(stargazer)\n#setwd(\"~/Dropbox (CSU Fullerton)/Econ340_R\")\ndata &lt;- read.csv(\"caschool.csv\")\n\n\n\nScatterplot\n\nggplot(data, aes(x=str, y=testscr)) +\n  geom_point() +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\nScatterplot\n\nggplot(data, aes(x=str, y=testscr)) +\n  geom_point(shape=1) + theme_classic() +\n  labs(x=\"Student-Teacher Ratio\", y=\"Test Score\")\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\nlm() is a function used to fit linear regression models\nSyntax: lm(y ~ x1 + x2 + ... , data = mydata)\nUseful to store it as an object\n\n\nmodel &lt;- lm(testscr ~ str, data)\n\n\nApply summary() function to the stored result from output\n\n\n\nRegression Output\n\nsummary(model)\n\n\n\n\nRegression Output\n\nFitted model: \\[ \\hat{testscr} = 698.93 - 2.28 \\cdot str  \\]\n\\(R^2 = 0.05\\) implies that 5% of variation in test scores explained by student teacher ratio\nStandard errors (deviations): \\[ SE_{\\hat{\\beta_0}} = 9.47, \\quad SE_{\\hat{\\beta_1}} = 0.48 \\]\n\n\n\nRegression Output\n\nOften interested in testing the hypothesis: \\(H_0: \\beta_1 = 0 \\text{ against } H_1: \\beta_1 \\neq 0\\)\nCorresponding t-value: \\[ t_0 = \\frac{\\hat{\\beta_1}}{SE_{\\hat{\\beta_1}}} = \\frac{- 2.28}{0.48} = -4.75\\]\np-value: \\(p = 2 Pr(Z&gt;t_0)\\)\nIf \\(p&lt;\\alpha\\), coefficient significant at \\(\\alpha \\%\\) level of significance\n\n\n\nConfidence Intervals\n\n\\((1-\\alpha)\\%\\) confidence interval is given by: \\(\\hat{\\beta_1} \\pm z_{\\alpha/2} \\cdot SE_{\\hat{\\beta_1}}\\)\nNote that \\(z_{0.025}=1.96\\), so the 95% confidence interval: \\[-2.28 \\pm 1.96 \\cdot 0.48\\]\n::: {.cell layout-align=“center”}\nconfint(model)\n::: {.cell-output .cell-output-stdout} 2.5 %     97.5 %   (Intercept) 680.32313 717.542779   str          -3.22298  -1.336637 ::: :::\n\n\n\nPredicted and Residual Values\n\ndata$yhat &lt;- predict(model)\ndata$uhat &lt;- residuals(model)\n\nShould the average of testscr and yhat be the same?\n\nmean(data$testscr)\nmean(data$yhat)\n\nWhat should be the average of uhat?\n\nmean(data$uhat)\n\n\n\nPredicted and Residual Values\nWhat is the predicted value when str=21?\n\ndata %&gt;% select(testscr, str, yhat, uhat) %&gt;% \n  filter(str==21)\n\n  testscr str    yhat      uhat\n1   616.3  21 651.057 -34.75699\n\n\nRemember: \\[ \\hat{testscr} = 698.93 - 2.28 \\cdot str  \\] Note that: \\(\\hat{u_i} = Y_i-\\hat{Y}_i\\)\n\n\nPlotting the Fitted Line\n\nggplot(data, aes(x=str, y=testscr)) +\n  geom_point(shape=1) + theme_classic() +\n  geom_line(aes(y=yhat)) \n\n\n\n\n\n\n\n\n\n\nOutput using Stargazer\n\nstargazer(model, type=\"text\", \n          keep.stat = c(\"n\", \"adj.rsq\"))\n\n\n\nOutput from Multiple Models\n\nmodel1 &lt;- lm(math_scr ~ str, data)\nmodel2 &lt;- lm(read_scr ~ str, data)\nstargazer(model1, model2, type=\"text\", \n          keep.stat = c(\"n\", \"adj.rsq\"))\n\n\n\nOutput from Multiple Models\n\n\n\nMultiple Regression Model\n\nmodel3 &lt;- lm(testscr ~ str + comp_stu, data)\nstargazer(model, model3, type=\"text\", \n          keep.stat = c(\"n\", \"adj.rsq\"))\n\n\nNote: Use the adjusted \\(R^2\\) to compare two models with different number of variables\n\n\n\nMultiple Regression Model\n\n\n\nOmitted Variable Bias\n\nNegative coefficient on str smaller in magnitude after controlling for comp_stu\nLower comp_stu \\(\\rightarrow\\) Lower testscr\nLower comp_stu \\(\\leftrightarrow\\) Higher str\nSo comp_stu explains some of the relationship between str and testscr\n\n\n\nOmitted Variable Bias\n\n\n\n\n\n\n\n\n\n\n\nNext Class\n\nFor the next class download and load acs2019 dataset from the Dropbox folder\nWe will continue with linear regression in R\nCome prepared so we can start quickly"
  },
  {
    "objectID": "Lectures/Lecture 6-8/Slides6.html",
    "href": "Lectures/Lecture 6-8/Slides6.html",
    "title": "ECON 340\nEconomics Research Methods",
    "section": "",
    "text": "Before we begin\n\nMake sure you have R and RStudio installed on your computer\nLet’s create a new folder on our computers, you can call it Econ340_R\nNow let’s download the dataset “caschool.csv” from our Dropbox folder and save it in this folder.\n\n\n\nAbout R and R Studio\n\nR is an open-source language designed for statistical computing\nNumerous add-on packages are available for R\nOnce you install R, you will have base packages installed. We will also use other packages and install them as we go.\nRStudio IDE is a set of integrated tools designed to use R more easily\n\n\n\nInterface\n\n\n\nGetting Started\n\nOpen RStudio and create a new R Script:\nFile \\(\\rightarrow\\) New File \\(\\rightarrow\\) R Script\nThis should create a new untitled file in your window.\nSave this file to the Econ340_R folder we created:\nFile \\(\\rightarrow\\) Save as \\(\\rightarrow\\) Type getting_started \\(\\rightarrow\\) Save\nWe will write all of our code in this script and execute it using the Run button on the top right\n\n\n\nInstalling Packages\n\nLet us now install our first package TidyVerse\nTidyVerse is a collection of R packages that share an underlying design philosophy, grammar, and data structures\nYou only need to install a package once, so no need to do it again if you installed TidyVerse before\nTo install a package: Tools \\(\\rightarrow\\) Install Packages \\(\\rightarrow\\) Install from Repository (CRAN) \\(\\rightarrow\\) Type TidyVerse\n\n\n\nLoading Packages\n\nOnce a package is installed you need to load it before you can use it, so at the top of your R script put the following command:\n\nlibrary(tidyverse)\n\n\n\n\nFile Management\n\nIt is good practice to keep all files related to a project in one folder\nEssentially you want R to use this folder as your working directory\nWorking directory is the folder where R will save files and retrieve files from\n\n\n\nHow to set the working directory\nThree ways to set the working directory:\n\nOption 1: Initialize R from this folder: click on the R script in the folder to open RStudio\n\nNote: This doesn’t work if RStudio is already running so quit it and then restart it again by clicking on the R script in the folder.\n\nOption 2: Open the folder under Files on bottom-left in R Studio and click on: More \\(\\rightarrow\\) Set As Working Directory\nOption 3: Manually tell R the directory (path) of this folder\n\n\n\nOption 3: Manually setting the path\n\n# Manually set directory\nsetwd(\"/Users/dbhagia/myfolder\")\n\n\nOn a Mac, you can right click on any folder and click Get Info to get the path\nOn a Windows computer, you can find the location in the address bar on the top (replace “\\” with “/”)\nAlternatively, you can find your path by opening the folder under Files on bottom-left in R Studio and clicking: More \\(\\rightarrow\\) Copy Folder Path to Clipboard\n\n\n\nImporting Data\n\nWe can import data from all kinds of format in R\nSome of the common formats in which data are stored are .xls, .xlsx, .csv\n.xls and .xlsx are Microsoft Excel’s native formats, however often data is stored in .csv files as the they are simpler\nData in R format has extension .rda or .Rdata\n\n\n\nSo Far\n\nYou should have a folder on your computer with “caschool.csv” and “getting_started.R”.\nIn your R script, you shoud have the following commands\n::: {.cell layout-align=“center”}\nlibrary(tidyverse)\nsetwd(\"your_folder_location\") # Optional\n:::\nRun your code and make sure you do not get an error\nYou can select both lines and Run or use the dropdown next to Run and click Run All or use Source. (If you just click Run, it will only execute the current line)\n\n\n\nImporting Data\nTo import the dataset:\n\ndata &lt;- read.csv(\"caschool.csv\")\n\nThis data set consists of information on 420 elementary school districts in California from 1998-1999.  \nYou can find the description of the variables in the accompanying codebook.\n\n\nAssignment Operator\n\nThe symbol \\(&lt;-\\) stands for the assignment operator\nYou can use shorcut Alt + - (Windows) or Option + - (Mac)\n\n\ndata &lt;- read.csv(\"caschool.csv\")\n\nIn the above code, we created a new object data and assigned the dataset we loaded using read.csv() to this object\nIf your command ran succesfully, a new object called data should appear under the Environment (top-right)\n\n\n\nAssignment Operator\n\numm &lt;- 2\numm &lt;- \"Hello\"\numm\n\n[1] \"Hello\"\n\n\n\nThe first line of code above creates a new object umm and assigns value 2 to it.\nThe next line of code takes the existing object umm and assigns a new value to it.\nThird line displays what is stored in object umm\n\n\n\nExploring Data\n\nBefore doing anything too complicated, let’s get a feel of the data\nYou can double click on the object data (or whatever you named it) under the Environment (top-right)\nOr single click on the arrow on left of data to see the structure of the data (alternatively use str() command)\nSome variables are stored as characters (chr), some as integers (int)\n\n\n\nExploring Data\nTo see the list of variables:\n\nls(data)\n\nTo summarize all variables in a dataset:\n\nsummary(data)\n\nTo summarize a particular variable:\n\nsummary(data$avginc)\n\n\n\nR Syntax\n\nTo call a variable we need to use data$var_name as multiple data objects can be loaded in R at the same time\nSay we want to find the average math score\n\n# Average math score\nmean(data$math_scr)\n\n[1] 653.3426\n\n\nCan write comments using #\nTry median() and sd() as well\n\n\n\nHelp in R\n\nTo learn more about any function or it’s arguments, one can type ?function_name in the console.\nFor instance,\n\n?mean\n\nThis will open up the documentation for this section in the Help window (bottom-right).\n\n\n\nMore on R Syntax\nNearly everything we do in R fits into one of three categories:\n\nCreate or overwrite an object (using the assignment operator \\(&lt;-\\))\nApply functions to objects\nLook at objects\n\nWhen we input arguments in the order the function lists them, no need to explicitly specify what argument we are referring to.\n\n\nExercise for you\n\nCreate a new object called mean_comp that contains the mean of variable computer\nCreate a new variable in the data frame data called mean_comp that contains the mean of computer\n\n\n\nCreating Objects\n\nYou can create a new vector that contains numbers 1-5:\n::: {.cell layout-align=“center”}\nx1 &lt;- c(1, 2, 3, 4, 5)\n:::\nTo create x2 that contains 0 and 100 on the left and right of x1, respectively\n::: {.cell layout-align=“center”}\nx2 &lt;- c(0, 1, 2, 3, 4, 5, 100)\n:::\nAlternatively\n::: {.cell layout-align=“center”}\nx2 &lt;- c(0, x1, 100)\n:::\n\n\n\nAnother exercise for you\n\nUse ?mean() to figure out what the following code does:\n::: {.cell layout-align=“center”}\nmean(x2, 0.25)\n:::\n\n\n\nMore on R Syntax\n\nNote that mean(x2, 0.25) is equivalent to\n::: {.cell layout-align=“center”}\nmean(x = x2, trim = 0.25)\n::: {.cell-output .cell-output-stdout} [1] 3 ::: :::\nWe can even write:\n::: {.cell layout-align=“center”}\nmean(trim = 0.25, x = x2)\n::: {.cell-output .cell-output-stdout} [1] 3 ::: :::\n\nIf we follow the order, no need to explicitly refer to which argument and we can simply write mean(x2, 0.25).\n\n\nFew Last Words\n\nBest way to learn a programming language is by using it\nIt is definitely challenging with a steep learning curve, but it is rewarding in the end\nInternet is your friend (like you didn’t know that!)\nChatGPT is helpful, but you still need to understand the programming language to reap its benefits.\nHave fun while you are at it!\nNext class: really cool things you can do in R :)"
  },
  {
    "objectID": "Problem-Sets/PS2/PS2_Solution/ps2_solution.html",
    "href": "Problem-Sets/PS2/PS2_Solution/ps2_solution.html",
    "title": "Problem Set 2 Solution",
    "section": "",
    "text": "# Housekeeping \nlibrary(tidyverse) \nlibrary(readxl) \n\n# 1. Import data\necon_data &lt;- read_excel(\"econData.xlsx\")\nnbd_data &lt;- read.csv(\"nbd_data.csv\")\n\n# 2. Average medHHinc and parks\necon_data %&gt;% summarise(mean = mean(medHHinc))\nmean(nbd_data$parks)\n\n# 3. Merge data\nmerged_data &lt;- merge(econ_data, nbd_data, by=\"state\")\n\n# 4. Remove missing values\nmerged_data %&gt;% filter(is.na(poverty)==TRUE) # Alaska and Hawaii have missing values\nmerged_data &lt;- merged_data %&gt;% filter(is.na(poverty)==FALSE)\n\n# 5. State with the highest poverty rate is Tennessee\nmerged_data %&gt;% filter(poverty==max(poverty))\n\n# 6. Saving data in R format\nsave(merged_data, file=\"merged_data.rda\")\n\n# 7. Create log_hhinc\nmerged_data &lt;- merged_data %&gt;% \n  mutate(log_hhinc = log(medHHinc))\n\n# 8. Scatter plot\nggplot(merged_data, aes(x = log_hhinc, y=fine_partc_mttr)) +\n  geom_point(color=\"orange\") +\n  theme_classic()\n\n# 9. Create a new variable called high_income \nmerged_data &lt;- merged_data %&gt;% \n  mutate(high_income = ifelse(medHHinc&gt;45, 1, 0)) \nmerged_data %&gt;% count(high_income) # 22 high income states\n\n# 10. Average particulate matter by high_income\nmerged_data %&gt;% \n  group_by(high_income) %&gt;% \n  summarise(mean = mean(fine_partc_mttr))"
  }
]